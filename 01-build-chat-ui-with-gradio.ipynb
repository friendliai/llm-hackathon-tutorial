{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1CLccGYsUDH"
      },
      "source": [
        "\n",
        "# **Build a Chat UI with `gradio`**\n",
        "\n",
        "본 튜토리얼에서는 빠르고 간단하게 ML 모델을 위한 웹 인터페이스를 제작 및 공유할 수 있는 Python 라이브러리인 gradio에 대해 소개합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfx4GBMJvbsJ"
      },
      "outputs": [],
      "source": [
        "!pip install -U gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypH8gGPpuqK0"
      },
      "source": [
        "### **1. Quick Start with Chat Interface**\n",
        "\n",
        "gradio에서는 아주 간단하게 chatbot UI를 제작할 수 있도록, `ChatInterface` 클래스를 제공하고 있습니다.\n",
        "\n",
        "먼저, 사용자가 입력한 메세지를 똑같이 따라하는 chatbot을 만드는 것부터 시작해볼까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "AWVlwBKIrKRd",
        "outputId": "3dbe78be-e466-48d4-9c75-d7fa084bbe9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://8f917fb0a1a55b7465.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://8f917fb0a1a55b7465.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def echo(message, history):\n",
        "    return \"You typed: \" + message\n",
        "\n",
        "gr.ChatInterface(echo).launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCo367vJwPjV"
      },
      "source": [
        "\n",
        "> `gr.ChatInterface(echo).launch()`\n",
        "\n",
        "위의 코드 한 줄로 깔끔한 chatbot UI가 만들어졌습니다. 좀 더 기능을 추가해볼까요?\n",
        "\n",
        "\n",
        "만약 아주 긴 출력을 생성하는 chatbot을 만든다면, 모든 토큰을 처리한 후 한 번에 응답을 반환하는 것보다는 **원하는 출력 단위별로 응답을 보여주는 것(Streaming)**이 좋은 선택일 수 있습니다. 해당 기능을 구현하려면 다음과 같이 `yield`를 사용해 chat function을 작성하면 됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "6jjx8B5Ertnr",
        "outputId": "84c4f102-4645-4668-c85e-d114a568948a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://48040626289097d6b0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://48040626289097d6b0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def stream_echo(message, history):\n",
        "    for i in range(len(message)):\n",
        "      time.sleep(0.01)\n",
        "      yield \"You typed: \" + message[:i+1]\n",
        "\n",
        "gr.ChatInterface(stream_echo).launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yth4FFWUCA4I"
      },
      "source": [
        "예시 문장:\n",
        "\n",
        "Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitary Python function. You can then share a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. No JavaScript, CSS, or web hosting experience needed!\n",
        "You'll notice that in order to make your first demo, you created an instance of the gr.Interface class. The Interface class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVmYTmPdDp9B"
      },
      "source": [
        "### **2. Integrate Friendli Serverless Endpoint**\n",
        "\n",
        "이번에는 chat function을 Friendli Serverless Endpoint를 사용해 구현해보겠습니다. 사용할 모델은 [meta의 Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)입니다.\n",
        "\n",
        "chat template:\n",
        "\n",
        "```\n",
        "{% set loop_messages = messages %}\n",
        "  {% for message in loop_messages %}\n",
        "    {% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n",
        "    {% if loop.index0 == 0 %}\n",
        "      {% set content = bos_token + content %}\n",
        "    {% endif %}\n",
        "    {{ content }}\n",
        "  {% endfor %}\n",
        "  {% if add_generation_prompt %}\n",
        "    {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
        "  {% endif %}\n",
        "```\n",
        "\n",
        "request_body:\n",
        "```\n",
        "{\n",
        "  \"messages\" : [\n",
        "    {\n",
        "      \"role\" : \"user\",\n",
        "      \"content\" : \"Hello\"\n",
        "    },\n",
        "    {\n",
        "      \"role\" : \"assistant\",\n",
        "      \"content\" : \"Hello!, How can I help you?\"\n",
        "    },\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHLcoYp67PQO"
      },
      "source": [
        "튜토리얼 진행을 위해 Friendli Personal Access Token이 필요합니다.\n",
        "\n",
        "다음 링크를 따라 미리 토큰을 발급한 후 아래의 `FRIENDLI_TOKEN`의 값을 채워주세요.\n",
        "https://docs.friendli.ai/guides/serverless_endpoints/quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDNzybJQKp-O"
      },
      "source": [
        "Friendli Serverless Endpoint를 쉽게 사용하기 위해서,\n",
        "\n",
        "Python package인 `friendli-client`를 설치하고, 해당 package에서 제공하는 chat API를 사용하겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUoeftloF1fu"
      },
      "outputs": [],
      "source": [
        "!pip install friendli-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "qYBLiLxdEIGo",
        "outputId": "754a6d4f-137b-4331-b989-55d96932cfdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://095f6e10d2c070bcca.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://095f6e10d2c070bcca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from friendli import Friendli\n",
        "\n",
        "client = Friendli(token=FRIENDLI_TOKEN)\n",
        "\n",
        "def chat_function(message, history):\n",
        "  new_messages = []\n",
        "  for user, chatbot in history:\n",
        "    new_messages.append({\"role\" : \"user\", \"content\": user})\n",
        "    new_messages.append({\"role\" : \"assistant\", \"content\": chatbot})\n",
        "  new_messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "  stream = client.chat.completions.create(\n",
        "    model=\"meta-llama-3-70b-instruct\",\n",
        "    messages=new_messages,\n",
        "    stream=True\n",
        "  )\n",
        "  res = \"\"\n",
        "  for chunk in stream:\n",
        "    res += chunk.choices[0].delta.content or \"\"\n",
        "    yield res\n",
        "\n",
        "gr.ChatInterface(chat_function).launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwhNFLECMcWQ"
      },
      "source": [
        "# **3. Share your app with gradio**\n",
        "\n",
        "gradio를 사용하면 아주 간단한 방식으로 구현한 chatbot이나 ML 모델 app을 공유할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqmwInouM_5n"
      },
      "source": [
        "## 1. Generate Temporary Link\n",
        "\n",
        "가장 간단한 방법으로, 코드의 한 줄만 변경해서 72시간 동안 공유할 수 있는 링크를 제공합니다.\n",
        "- 해당 링크는 로컬에서 프로그램이 실행 중일 때만 유효합니다\n",
        "- 해당 링크로 어떤 사람이든 접근할 수 있습니다. 보안 문제가 있다면 다음 섹션을 참고해주세요.\n",
        "\n",
        "> Jupyter notebook을 사용하는 경우, defualt로 링크를 생성합니다. 그 외에는 `launch(share=True)`를 명시해주어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "wWiY3gDqIK8K",
        "outputId": "b50fa574-0d8e-4ba1-8d2e-04a4ee8c39c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://6ea486555c992baa6b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://6ea486555c992baa6b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from friendli import Friendli\n",
        "\n",
        "client = Friendli(token=FRIENDLI_TOKEN)\n",
        "\n",
        "def chat_function(message, history):\n",
        "  new_messages = []\n",
        "  for user, chatbot in history:\n",
        "    new_messages.append({\"role\" : \"user\", \"content\": user})\n",
        "    new_messages.append({\"role\" : \"assistant\", \"content\": chatbot})\n",
        "  new_messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "  stream = client.chat.completions.create(\n",
        "    model=\"meta-llama-3-70b-instruct\",\n",
        "    messages=new_messages,\n",
        "    stream=True\n",
        "  )\n",
        "  res = \"\"\n",
        "  for chunk in stream:\n",
        "    res += chunk.choices[0].delta.content or \"\"\n",
        "    yield res\n",
        "\n",
        "# gr.ChatInterface(chat_function).launch()\n",
        "gr.ChatInterface(chat_function).launch(share=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etUX99SjMvtu"
      },
      "source": [
        "## 2. Hosting on Huggingface Space\n",
        "\n",
        "영구적으로 app을 호스팅하고 싶다면, [Huggingface Space](https://huggingface.co/spaces)을 사용할 수 있습니다.\n",
        "\n",
        "튜토리얼 진행을 위해서\n",
        "- space 생성을 위한 [Huggingface](https://huggingface.co/) 계정이 필요합니다.\n",
        "- Jupyter Notebook 파일이 아닌 파이썬 파일(.py)로 작성된 프로그램이 필요합니다.\n",
        "\n",
        "다음 파일을 다운받아 로컬 환경에서 진행해주세요\n",
        "- https://drive.google.com/file/d/1XmV0Kh8RjKJ9KShf4wY50XJapBZNmxt6/view?usp=sharing\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
