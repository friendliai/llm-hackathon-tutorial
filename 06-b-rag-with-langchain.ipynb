{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this guide, we will build a simple RAG-based chatbot that answers questions about the contents of a PDF document. This tutorial will use Friendli Serverless Endpoints for LLM inference and MongoDB Atlas for the vector store.",
   "id": "72e0e8c6f29d30c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dependencies\n",
    "\n",
    "First, let’s install the required packages:"
   ],
   "id": "59bf42c64ae5c98a"
  },
  {
   "cell_type": "code",
   "source": "!pip install langchain langchain-community friendli-client pypdf pymongo langchain-openai tiktoken",
   "metadata": {
    "collapsed": false
   },
   "id": "17277990f018392c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up MongoDB Atlas\n",
    "While you can run MongoDB locally, this tutorial will use MongoDB Atlas, a managed service.\n",
    "We have prepared a dedicated cluster for each team, so you can use it.\n",
    "\n",
    "With the provided connection info, initialize the MongoDB client:"
   ],
   "id": "4bdc857b56b208bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:22:08.302619Z",
     "start_time": "2024-05-22T08:22:08.172904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "MONGODB_ATLAS_CLUSTER_URI = \"YOUR_MONGO_URI\"  # mongodb+srv://team-xx:<password>@cluster0.tyqdayd.mongodb.net/\n",
    "\n",
    "client = MongoClient(MONGODB_ATLAS_CLUSTER_URI)\n",
    "\n",
    "DB_NAME = \"team-xx\"\n",
    "COLLECTION_NAME = \"rag\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"team_xx_rag\"\n",
    "\n",
    "MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the connection by running:",
   "id": "d0ea167c3df53bc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "client.server_info()",
   "id": "64e959f8553a6399",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating a Vector Search Index\n",
    "\n",
    "To use MongoDB as a vector store, you need to create a vector search index for querying. Configure the search index as follows:"
   ],
   "id": "ab48e8e79935461f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "search_model = SearchIndexModel(\n",
    "    definition={\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"numDimensions\": 1536,\n",
    "                \"path\": \"embedding\",\n",
    "                \"similarity\": \"cosine\",\n",
    "                \"type\": \"vector\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    type=\"vectorSearch\",\n",
    ")\n",
    "client[DB_NAME][COLLECTION_NAME].create_search_index(search_model)"
   ],
   "id": "e83f40a145c4c8de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading Documents and Embeddings\n",
    "\n",
    "Now, let’s load a document from a PDF file and insert them into MongoDB Atlas with their embeddings. In our case, we’ll load the BPipe paper from the ICML 2023 conference:"
   ],
   "id": "bd2c21545147c24a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:22:14.428911Z",
     "start_time": "2024-05-22T08:22:09.546630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"https://openreview.net/pdf?id=HVKmLi1iR4\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=OpenAIEmbeddings(disallowed_special=()),\n",
    "    collection=MONGODB_COLLECTION,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    ")\n",
    "retriever = vector_store.as_retriever()"
   ],
   "id": "ba8958c1935e885d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initializing the LLM with Friendli\n",
    "\n",
    "Now, let’s initialize the LLM part using Friendli Serverless Endpoints, using Meta’s new Llama 3 70B model:"
   ],
   "id": "4e980b17f2e7b689"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:22:14.561537Z",
     "start_time": "2024-05-22T08:22:14.431254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models.friendli import ChatFriendli\n",
    "\n",
    "llm = ChatFriendli(model=\"meta-llama-3-70b-instruct\")"
   ],
   "id": "bbbcc1a11f9c7073",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building the RAG Chain\n",
    "\n",
    "We have prepared all the components for our RAG pipeline. Here’s how to ask questions about the PDF file. In our case, we’ll find out what the ‘memory imbalance problem’ is, within BPipe’s context."
   ],
   "id": "97fa8c6af5295e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:22:52.210508Z",
     "start_time": "2024-05-22T08:22:49.169123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don’t know the answer, just say that you don’t know, don’t try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say “thanks for asking!” at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is the memory imbalance problem that BPipe solves?\")"
   ],
   "id": "fd61e394f7336bcc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The memory imbalance problem that BPIPE solves is when certain GPUs face high memory pressure while others underutilize their capacity during pipeline parallelism, resulting in suboptimal training performance. Thanks for asking!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Upon execution, you will be able to get the response from the RAG-applied model, which correctly describes the information from the pdf file, despite it being excluded from the data used to train the original model.",
   "id": "e6ad8d71c7002c27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
