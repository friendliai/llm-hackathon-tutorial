{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-21T07:17:02.619177Z",
     "start_time": "2024-05-21T07:17:01.013139Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install -qU langchain langchain-community friendli-client requests"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "FRIENDLI_TOKEN = \"flp_xxxx\"  # https://suite.friendli.ai/user-settings/tokens\n",
    "def retrieve_contexts(document_ids: list[str], query: str, k: int) -> list[str]:\n",
    "    resp = requests.post(\n",
    "        \"https://suite.friendli.ai/api/beta/retrieve\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {FRIENDLI_TOKEN}\",\n",
    "        },\n",
    "        json={\n",
    "            \"document_ids\": document_ids,\n",
    "            \"query\": query,\n",
    "            \"k\": k,\n",
    "        }\n",
    "    )\n",
    "    data = resp.json()\n",
    "    return [r[\"content\"] for r in data[\"results\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T07:17:02.655784Z",
     "start_time": "2024-05-21T07:17:02.621408Z"
    }
   },
   "id": "17277990f018392c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORCA: A Distributed Serving System for\\nTransformer-Based Generative Models\\nGyeong-In Yu\\nSeoul National UniversityJoo Seong Jeong\\nSeoul National UniversityGeon-Woo Kim\\nFriendliAI\\nSeoul National University\\nSoojeong Kim\\nFriendliAIByung-Gon Chun∗\\nFriendliAI\\nSeoul National University\\nAbstract\\nLarge-scale Transformer-based models trained for generation\\ntasks (e.g., GPT-3) have recently attracted huge interest, em-\\nphasizing the need for system support for serving models in\\nthis family. Since these models generate a next token in an au-\\ntoregressive manner, one has to run the model multiple times\\nto process an inference request where each iteration of the\\nmodel generates a single output token for the request. How-\\never, existing systems for inference serving do not perform\\nwell on this type of workload that has a multi-iteration char-\\nacteristic, due to their inﬂexible scheduling mechanism that\\ncannot change the current batch of requests being processed;\\nrequests that have ﬁnished earlier than other requests in a\\nbatch cannot return to the client, while newly arrived requests\\nhave to wait until the current batch completely ﬁnishes.\\nIn this paper, we propose iteration-level scheduling, a new\\nscheduling mechanism that schedules execution at the gran-\\nularity of iteration (instead of request) where the scheduler\\ninvokes the execution engine to run only a single iteration of\\nthe model on the batch. In addition, to apply batching and\\niteration-level scheduling to a Transformer model at the same\\ntime, we suggest selective batching, which applies batching\\nonly to a selected set of operations. Based on these two tech-\\nniques, we have implemented a distributed serving system\\ncalled ORCA, with additional designs for scalability to models\\nwith hundreds of billions of parameters. Our evaluation on a\\nGPT-3 175B model shows that ORCA can signiﬁcantly out-\\nperform NVIDIA FasterTransformer in terms of both latency\\nand throughput: 36 .9×throughput improvement at the same\\nlevel of latency.\\n1 Introduction\\nLanguage generation tasks are becoming increasingly\\nparamount to many types of applications, such as chatbot [9,\\n52], summarization [41,45,54], code generation [13], and cap-\\ntion generation [65,66]. Moreover, recent works published by\\n∗Corresponding author.AI21 Labs [37], DeepMind [26,48], Google [15,21,63], Meta\\nPlatforms [10,67], Microsoft [50], Microsoft & NVIDIA [59],\\nand OpenAI [12] have reported that every language process-\\ning task, including translation [11, 17], classiﬁcation [20, 53],\\nquestion-answering [32, 33, 40] and more, can be cast as a\\nlanguage generation problem and have shown great improve-\\nments along this direction. The rise of generative models is\\nnot limited to the language domain; the AI community has\\nalso given growing interest to generation problems in other do-\\nmains such as image, video, speech, or a mixture of multiple\\ndomains [19,38,51,62]. At the heart of generative models lies\\nthe Transformer architecture [60] and its variants [15, 47 –49].\\nBy relying on the attention mechanism [60], Transformer\\nmodels can learn better representations where each element\\nof the sequence may have a direct connection with every other\\nelement, which was not possible in recurrent models [25].\\nTo use generative models in real-world applications, we\\noften delegate the inference procedure to a separate service\\nresponsible for ML inference serving. The growing demands\\nfor this service, which should provide inference results for\\nclient requests at low latency and high throughput, have fa-\\ncilitated the development of inference serving systems such\\nas Triton Inference Server [7] and TensorFlow Serving [42].\\nThese systems can use a separately-developed DNN execution\\nengine to perform the actual tensor operations. For example,\\nwe can deploy a service for language generation tasks by\\nusing a combination of Triton and FasterTransformer [4], an\\nexecution engine optimized for the inference of Transformer-', 'ORCA: A Distributed Serving System for\\nTransformer-Based Generative Models\\nGyeong-In Yu\\nSeoul National UniversityJoo Seong Jeong\\nSeoul National UniversityGeon-Woo Kim\\nFriendliAI\\nSeoul National University\\nSoojeong Kim\\nFriendliAIByung-Gon Chun∗\\nFriendliAI\\nSeoul National University\\nAbstract\\nLarge-scale Transformer-based models trained for generation\\ntasks (e.g., GPT-3) have recently attracted huge interest, em-\\nphasizing the need for system support for serving models in\\nthis family. Since these models generate a next token in an au-\\ntoregressive manner, one has to run the model multiple times\\nto process an inference request where each iteration of the\\nmodel generates a single output token for the request. How-\\never, existing systems for inference serving do not perform\\nwell on this type of workload that has a multi-iteration char-\\nacteristic, due to their inﬂexible scheduling mechanism that\\ncannot change the current batch of requests being processed;\\nrequests that have ﬁnished earlier than other requests in a\\nbatch cannot return to the client, while newly arrived requests\\nhave to wait until the current batch completely ﬁnishes.\\nIn this paper, we propose iteration-level scheduling, a new\\nscheduling mechanism that schedules execution at the gran-\\nularity of iteration (instead of request) where the scheduler\\ninvokes the execution engine to run only a single iteration of\\nthe model on the batch. In addition, to apply batching and\\niteration-level scheduling to a Transformer model at the same\\ntime, we suggest selective batching, which applies batching\\nonly to a selected set of operations. Based on these two tech-\\nniques, we have implemented a distributed serving system\\ncalled ORCA, with additional designs for scalability to models\\nwith hundreds of billions of parameters. Our evaluation on a\\nGPT-3 175B model shows that ORCA can signiﬁcantly out-\\nperform NVIDIA FasterTransformer in terms of both latency\\nand throughput: 36 .9×throughput improvement at the same\\nlevel of latency.\\n1 Introduction\\nLanguage generation tasks are becoming increasingly\\nparamount to many types of applications, such as chatbot [9,\\n52], summarization [41,45,54], code generation [13], and cap-\\ntion generation [65,66]. Moreover, recent works published by\\n∗Corresponding author.AI21 Labs [37], DeepMind [26,48], Google [15,21,63], Meta\\nPlatforms [10,67], Microsoft [50], Microsoft & NVIDIA [59],\\nand OpenAI [12] have reported that every language process-\\ning task, including translation [11, 17], classiﬁcation [20, 53],\\nquestion-answering [32, 33, 40] and more, can be cast as a\\nlanguage generation problem and have shown great improve-\\nments along this direction. The rise of generative models is\\nnot limited to the language domain; the AI community has\\nalso given growing interest to generation problems in other do-\\nmains such as image, video, speech, or a mixture of multiple\\ndomains [19,38,51,62]. At the heart of generative models lies\\nthe Transformer architecture [60] and its variants [15, 47 –49].\\nBy relying on the attention mechanism [60], Transformer\\nmodels can learn better representations where each element\\nof the sequence may have a direct connection with every other\\nelement, which was not possible in recurrent models [25].\\nTo use generative models in real-world applications, we\\noften delegate the inference procedure to a separate service\\nresponsible for ML inference serving. The growing demands\\nfor this service, which should provide inference results for\\nclient requests at low latency and high throughput, have fa-\\ncilitated the development of inference serving systems such\\nas Triton Inference Server [7] and TensorFlow Serving [42].\\nThese systems can use a separately-developed DNN execution\\nengine to perform the actual tensor operations. For example,\\nwe can deploy a service for language generation tasks by\\nusing a combination of Triton and FasterTransformer [4], an\\nexecution engine optimized for the inference of Transformer-']\n"
     ]
    }
   ],
   "source": [
    "document_ids = [...]\n",
    "contexts = retrieve_contexts(document_ids, \"What is Orca?\", 2)\n",
    "print(contexts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T07:17:03.261082Z",
     "start_time": "2024-05-21T07:17:02.656319Z"
    }
   },
   "id": "2c3b75d1c224037f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33;20m2024-05-21 16:17:03,638.00638: Friendli WARNING: You've entered your login information in two places - through the 'FRIENDLI_TOKEN' environment variable and the 'friendli login' CLI command. We will use the access token from the 'FRIENDLI_TOKEN' environment variable and ignore the login session details. This might lead to unexpected authorization errors. If you prefer to use the login session instead, unset the 'FRIENDLI_TOKEN' environment variable. If you don't want to see this warning again, run 'friendli logout' to remove the login session.\u001B[0m\n",
      "\u001B[33;20m2024-05-21 16:17:03,639.00639: Friendli WARNING: You've entered your login information in two places - through the 'FRIENDLI_TOKEN' environment variable and the 'friendli login' CLI command. We will use the access token from the 'FRIENDLI_TOKEN' environment variable and ignore the login session details. This might lead to unexpected authorization errors. If you prefer to use the login session instead, unset the 'FRIENDLI_TOKEN' environment variable. If you don't want to see this warning again, run 'friendli logout' to remove the login session.\u001B[0m\n",
      "/Users/maro/Library/Caches/pypoetry/virtualenvs/llm-hackathon-tutorial-zNQqE4Ri-py3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.call_as_llm` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\u001B[33;20m2024-05-21 16:17:03,701.00701: Friendli WARNING: You've entered your login information in two places - through the 'FRIENDLI_TOKEN' environment variable and the 'friendli login' CLI command. We will use the access token from the 'FRIENDLI_TOKEN' environment variable and ignore the login session details. This might lead to unexpected authorization errors. If you prefer to use the login session instead, unset the 'FRIENDLI_TOKEN' environment variable. If you don't want to see this warning again, run 'friendli logout' to remove the login session.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Orca can refer to different things, but I'll cover the most common meanings:\\n\\n1. **Orca (killer whale)**: The orca, also known as the killer whale, is a toothed whale belonging to the oceanic dolphin family. It is the largest member of the dolphin family and is known for its distinctive black and white coloring. Orcas are apex predators, which means they have no natural predators in the wild. They are highly social, intelligent, and communicate using a variety of clicks, whistles, and body language.\\n2. **Orca (software)**: Orca is an open-source screen reader and assistive technology developed by the GNOME Project. It provides a way for people with visual impairments or blindness to interact with graphical user interfaces (GUIs) using a braille display or synthesized speech.\\n3. **Orca (Marvel Comics)**: Orca is a fictional character in the Marvel Comics universe. She is a supervillain and an enemy of Namor the Sub-Mariner. Her real name is Suzanna Sherman, and she gained her abilities through genetic engineering.\\n4. **Orca (other meanings)**: Orca can also refer to other things, such as:\\n\\t* Orca (investment firm): A private investment firm based in the United States.\\n\\t* Orca (beer): A beer brand from the Pacific Northwest region of the United States.\\n\\t* Orca ( boat): A type of boat or yacht designed for fishing or cruising.\\n\\nWithout more context, it's difficult to determine which Orca you are referring to. If you have any additional information or clarification, I'd be happy to try and provide a more specific answer!\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models.friendli import ChatFriendli\n",
    "\n",
    "llm = ChatFriendli(model=\"meta-llama-3-70b-instruct\", friendli_token=FRIENDLI_TOKEN)\n",
    "llm.call_as_llm(message=\"What is Orca?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T07:17:15.217184Z",
     "start_time": "2024-05-21T07:17:03.264751Z"
    }
   },
   "id": "701d8ba457489e8b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33;20m2024-05-21 16:17:15,225.00225: Friendli WARNING: You've entered your login information in two places - through the 'FRIENDLI_TOKEN' environment variable and the 'friendli login' CLI command. We will use the access token from the 'FRIENDLI_TOKEN' environment variable and ignore the login session details. This might lead to unexpected authorization errors. If you prefer to use the login session instead, unset the 'FRIENDLI_TOKEN' environment variable. If you don't want to see this warning again, run 'friendli logout' to remove the login session.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'ORCA is a distributed serving system for Transformer-based generative models. It is designed to provide low-latency and high-throughput inference serving for large-scale Transformer models, such as GPT-3. Thanks for asking!'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don’t know the answer, just say that you don’t know, don’t try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say “thanks for asking!” at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "rag_message = template.format(context=\"\\n\".join(contexts), question=\"What is Orca?\")\n",
    "llm.call_as_llm(message=rag_message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T07:17:18.079240Z",
     "start_time": "2024-05-21T07:17:15.221084Z"
    }
   },
   "id": "55fcee505c635769",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T07:17:18.081683Z",
     "start_time": "2024-05-21T07:17:18.080150Z"
    }
   },
   "id": "25dd6ef072263777",
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
